{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "875fcd95",
   "metadata": {},
   "source": [
    "# üåü Urban Comfort Potential from Open Environmental Data\n",
    "\n",
    "### Title:\n",
    "**A reproducible and explainable framework for assessing urban environmental comfort and spatial inequality using open data.**\n",
    "\n",
    "**Scope**\n",
    "\n",
    "This notebook implements a fully auditable pipeline to estimate **urban environmental comfort potential** using open datasets only. It builds four structural dimensions:\n",
    "\n",
    "* **Acoustic exposure** (road noise)\n",
    "* **Vegetation** (NDVI)\n",
    "* **Structural crowd pressure** (built and activity proxies)\n",
    "* **Infrastructure related accessibility** (access and support proxies)\n",
    "\n",
    "These dimensions are later combined into an interpretable **ComfortScore**.\n",
    "\n",
    "**Does not:**\n",
    "* Predict individual wellbeing\n",
    "* Use personal data, behavioural tracking, or surveys\n",
    "\n",
    "**Strategy**\n",
    "* Use OpenStreetMap venues to define the Area of Interest and a consistent measurement support\n",
    "* Acquire authoritative layers with cache aware scripts\n",
    "* Save outputs with clear paths and simple provenance logs\n",
    "\n",
    "## Notebook structure\n",
    "1. Setup and reproducibility settings\n",
    "2. Phase A Data acquisition\n",
    "3. Phase B Feature engineering\n",
    "4. Phase C ComfortScore model and explanation layer\n",
    "5. Phase D External data download\n",
    "6. Phase E External validation and spatial structure\n",
    "7. Phase-F Final synthesis and future directions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37110baf",
   "metadata": {},
   "source": [
    "**Setup and reproducibility settings**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e3e5930c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup complete\n"
     ]
    }
   ],
   "source": [
    "# Core\n",
    "import os\n",
    "import re\n",
    "import io\n",
    "import json\n",
    "import time\n",
    "import glob\n",
    "import zipfile\n",
    "import hashlib\n",
    "from pathlib import Path\n",
    "from datetime import date, datetime, timezone\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "import mgrs\n",
    "import rasterio\n",
    "from rasterio.mask import mask\n",
    "from rasterio.errors import RasterioIOError\n",
    "from requests.adapters import HTTPAdapter\n",
    "from urllib3.util.retry import Retry\n",
    "\n",
    "# Data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import requests\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "print(\"Setup complete\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "875f41bc",
   "metadata": {},
   "source": [
    "# üéØ Phase-A Data acquisition\n",
    "\n",
    "**Inputs acquired in Phase A**\n",
    "* **Venues** from OpenStreetMap to define the Area of Interest\n",
    "* **DEFRA road noise polygons** via the official OGC API Features endpoint\n",
    "* **Sentinel 2 NDVI tiles** from CEDA, combined into a summer median composite\n",
    "\n",
    "**Key reproducibility features**\n",
    "* All downloads are **cached locally** and skipped on rerun when valid.\n",
    "* Each dataset writes a **provenance JSON** with timestamps, sources, and key parameters.\n",
    "* Manual access instructions are printed so a reviewer can verify sources without guessing.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3d6a16b",
   "metadata": {},
   "source": [
    "## A0: Paths and study area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "debab117",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root: /home/sai/test/notebook_test\n",
      "Raw dir: /home/sai/test/notebook_test/data/raw\n",
      "Processed dir: /home/sai/test/notebook_test/data/processed\n",
      "Study area: ['London Borough of Camden', 'London Borough of Islington']\n"
     ]
    }
   ],
   "source": [
    "# We set a consistent folder structure used across all phases.\n",
    "\n",
    "# Folder convention:\n",
    "# `data/raw/` stores downloaded or source like files\n",
    "# `data/processed/` stores cleaned and standardised outputs used downstream\n",
    "\n",
    "# Notebook and project paths\n",
    "NOTEBOOK_DIR = Path(\".\").resolve()\n",
    "PROJECT_ROOT = NOTEBOOK_DIR.parent\n",
    "\n",
    "DATA_DIR = PROJECT_ROOT / \"data\"\n",
    "RAW_DIR = DATA_DIR / \"raw\"\n",
    "PROCESSED_DIR = DATA_DIR / \"processed\"\n",
    "\n",
    "RAW_DIR.mkdir(parents=True, exist_ok=True)\n",
    "PROCESSED_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Study area names as used by OSMnx\n",
    "PLACE_NAMES = [\"London Borough of Camden\", \"London Borough of Islington\"]\n",
    "\n",
    "# Venue outputs\n",
    "VENUES_RAW_PATH = RAW_DIR / \"venues_raw.geojson\"\n",
    "VENUES_CLEAN_PATH = PROCESSED_DIR / \"venues_clean.geojson\"\n",
    "\n",
    "print(\"Project root:\", PROJECT_ROOT)\n",
    "print(\"Raw dir:\", RAW_DIR)\n",
    "print(\"Processed dir:\", PROCESSED_DIR)\n",
    "print(\"Study area:\", PLACE_NAMES)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59eb09cc",
   "metadata": {},
   "source": [
    "## A1: Venues from OpenStreetMap\n",
    "\n",
    "We start with a set of candidate venues.\n",
    "\n",
    "Why venues first:\n",
    "* They define the **spatial footprint** of where we measure comfort.\n",
    "* Later features (noise, vegetation, roads, accessibility) are computed around each venue.\n",
    "* This keeps the pipeline place based and avoids individual level inference.\n",
    "\n",
    "Outputs:\n",
    "* `data/raw/venues_raw.geojson`\n",
    "* `data/processed/venues_clean.geojson`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd0483ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OSMnx available: True\n"
     ]
    }
   ],
   "source": [
    "# Try to import OSMnx\n",
    "try:\n",
    "    import osmnx as ox\n",
    "    OSMNX_OK = True\n",
    "except Exception as e:\n",
    "    OSMNX_OK = False\n",
    "    OSMNX_ERR = str(e)\n",
    "\n",
    "print(\"OSMnx available:\", OSMNX_OK)\n",
    "if not OSMNX_OK:\n",
    "    print(\"Import error:\", OSMNX_ERR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21f1ef94",
   "metadata": {},
   "source": [
    "### A1.1: Venue query and cleaning rules\n",
    "\n",
    "We use a small set of OSM tags that match common peaceful or inclusive places.\n",
    "\n",
    "Cleaning rules:\n",
    "* Reproject to British National Grid for consistent distance calculations\n",
    "* Keep a small set of stable columns\n",
    "* Drop empty names to avoid unlabeled fragments\n",
    "* Drop invalid geometries\n",
    "* Deduplicate by OSM id\n",
    "* Add representative point coordinates `x` and `y` for convenience\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dde26066",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tags define what we treat as candidate venues\n",
    "VENUE_TAGS = {\n",
    "    \"leisure\": [\"park\", \"garden\", \"nature_reserve\"],\n",
    "    \"amenity\": [\"library\", \"cafe\", \"community_centre\", \"arts_centre\", \"museum\"],\n",
    "    \"landuse\": [\"grass\", \"recreation_ground\"],\n",
    "}\n",
    "\n",
    "# British National Grid for metre based buffers\n",
    "TARGET_CRS = \"EPSG:27700\"\n",
    "\n",
    "def fetch_venues_osm(place_names, tags):\n",
    "    \"\"\"Download OSM venues using OSMnx.\"\"\"\n",
    "    if not OSMNX_OK:\n",
    "        raise ImportError(\"OSMnx is not available. Install osmnx to run this cell.\")\n",
    "\n",
    "    # OSMnx caching helps reproducibility and avoids repeated network calls\n",
    "    ox.settings.log_console = True\n",
    "    ox.settings.use_cache = True\n",
    "\n",
    "    print(\"Fetching OSM venues for:\", place_names)\n",
    "    t0 = time.time()\n",
    "    gdf = ox.features_from_place(place_names, tags=tags)\n",
    "    gdf = gdf.reset_index()\n",
    "    print(\"Fetched rows:\", len(gdf), \"in\", round(time.time() - t0, 2), \"seconds\")\n",
    "    return gdf\n",
    "\n",
    "def clean_venues_osm(gdf):\n",
    "    \"\"\"Clean raw OSM venues for stable downstream use.\"\"\"\n",
    "    gdf = gdf.copy()\n",
    "\n",
    "    # CRS handling\n",
    "    if gdf.crs is None:\n",
    "        gdf = gdf.set_crs(\"EPSG:4326\")\n",
    "    gdf = gdf.to_crs(TARGET_CRS)\n",
    "\n",
    "    # Stable id\n",
    "    if \"id\" not in gdf.columns:\n",
    "        if \"osmid\" in gdf.columns:\n",
    "            gdf[\"id\"] = gdf[\"osmid\"]\n",
    "        else:\n",
    "            gdf[\"id\"] = np.arange(len(gdf))\n",
    "\n",
    "    keep_cols = [\n",
    "        \"id\", \"name\", \"amenity\", \"leisure\", \"landuse\",\n",
    "        \"wheelchair\", \"opening_hours\", \"operator\", \"geometry\"\n",
    "    ]\n",
    "    keep_cols = [c for c in keep_cols if c in gdf.columns]\n",
    "    gdf = gdf[keep_cols].copy()\n",
    "\n",
    "    # Name cleaning\n",
    "    gdf[\"name\"] = gdf[\"name\"].fillna(\"\").astype(str).str.strip()\n",
    "    mask_valid = gdf[\"name\"].apply(lambda x: len(x) > 0 and x.lower() not in [\"nan\", \"none\"])\n",
    "    gdf = gdf[mask_valid].copy()\n",
    "\n",
    "    # Geometry cleaning\n",
    "    gdf = gdf[gdf.geometry.notna() & ~gdf.geometry.is_empty].copy()\n",
    "\n",
    "    # Deduplicate\n",
    "    gdf[\"id\"] = gdf[\"id\"].astype(str)\n",
    "    gdf = gdf.drop_duplicates(subset=[\"id\"]).copy()\n",
    "\n",
    "    # Representative point is safe for polygons and multipolygons\n",
    "    rep = gdf.geometry.representative_point()\n",
    "    gdf[\"x\"] = rep.x\n",
    "    gdf[\"y\"] = rep.y\n",
    "\n",
    "    print(\"Cleaned venues:\", len(gdf))\n",
    "    print(\"CRS:\", gdf.crs)\n",
    "    return gdf\n",
    "\n",
    "def save_geojson(gdf, path):\n",
    "    \"\"\"Save GeoDataFrame to GeoJSON with folders created if needed.\"\"\"\n",
    "    path = Path(path)\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    gdf.to_file(path, driver=\"GeoJSON\")\n",
    "    print(\"Saved:\", str(path))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a92915b6",
   "metadata": {},
   "source": [
    "### A1.2: Run venue acquisition\n",
    "\n",
    "Cache behaviour:\n",
    "* If `venues_clean.geojson` exists and is non empty, we reuse it.\n",
    "* Otherwise we fetch from OSM, save a raw copy, then save a cleaned version.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d4331e2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching OSM venues for: ['London Borough of Camden', 'London Borough of Islington']\n",
      "Fetched rows: 2555 in 42.01 seconds\n",
      "Saved: /home/sai/test/notebook_test/data/raw/venues_raw.geojson\n",
      "Cleaned venues: 1211\n",
      "CRS: EPSG:27700\n",
      "Saved: /home/sai/test/notebook_test/data/processed/venues_clean.geojson\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "      <th>amenity</th>\n",
       "      <th>leisure</th>\n",
       "      <th>landuse</th>\n",
       "      <th>wheelchair</th>\n",
       "      <th>opening_hours</th>\n",
       "      <th>operator</th>\n",
       "      <th>geometry</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>13799212</td>\n",
       "      <td>Holborn Library</td>\n",
       "      <td>library</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>yes</td>\n",
       "      <td>Mo-Th 10:00-19:00, Fr 10:00-17:00, Sa 11:00-17:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>POINT (530835.223 181922.569)</td>\n",
       "      <td>530835.223366</td>\n",
       "      <td>181922.569174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>25475389</td>\n",
       "      <td>Woburn Cafe</td>\n",
       "      <td>cafe</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>POINT (529873.562 182510.483)</td>\n",
       "      <td>529873.561987</td>\n",
       "      <td>182510.483335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>25497832</td>\n",
       "      <td>Cafe Angel</td>\n",
       "      <td>cafe</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>POINT (530790.094 182331.853)</td>\n",
       "      <td>530790.093853</td>\n",
       "      <td>182331.853236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>248231088</td>\n",
       "      <td>Kalendar</td>\n",
       "      <td>cafe</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>POINT (528356.799 186417.814)</td>\n",
       "      <td>528356.798695</td>\n",
       "      <td>186417.813973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>267971989</td>\n",
       "      <td>The Brew House Caf√©</td>\n",
       "      <td>cafe</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Dec-Jan: Mo-Su 09:00-16:00; Feb-Mar: Mo-Su 09:...</td>\n",
       "      <td>English Heritage</td>\n",
       "      <td>POINT (527143.833 187468.195)</td>\n",
       "      <td>527143.832630</td>\n",
       "      <td>187468.194588</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id                 name  amenity leisure landuse wheelchair  \\\n",
       "0   13799212      Holborn Library  library     NaN     NaN        yes   \n",
       "1   25475389          Woburn Cafe     cafe     NaN     NaN        NaN   \n",
       "2   25497832           Cafe Angel     cafe     NaN     NaN        NaN   \n",
       "3  248231088             Kalendar     cafe     NaN     NaN        NaN   \n",
       "4  267971989  The Brew House Caf√©     cafe     NaN     NaN        NaN   \n",
       "\n",
       "                                       opening_hours          operator  \\\n",
       "0  Mo-Th 10:00-19:00, Fr 10:00-17:00, Sa 11:00-17:00               NaN   \n",
       "1                                                NaN               NaN   \n",
       "2                                                NaN               NaN   \n",
       "3                                                NaN               NaN   \n",
       "4  Dec-Jan: Mo-Su 09:00-16:00; Feb-Mar: Mo-Su 09:...  English Heritage   \n",
       "\n",
       "                        geometry              x              y  \n",
       "0  POINT (530835.223 181922.569)  530835.223366  181922.569174  \n",
       "1  POINT (529873.562 182510.483)  529873.561987  182510.483335  \n",
       "2  POINT (530790.094 182331.853)  530790.093853  182331.853236  \n",
       "3  POINT (528356.799 186417.814)  528356.798695  186417.813973  \n",
       "4  POINT (527143.833 187468.195)  527143.832630  187468.194588  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if VENUES_CLEAN_PATH.exists() and VENUES_CLEAN_PATH.stat().st_size > 0:\n",
    "    print(\"Venues already present:\", VENUES_CLEAN_PATH)\n",
    "    venues = gpd.read_file(VENUES_CLEAN_PATH)\n",
    "    print(\"Loaded venues:\", len(venues))\n",
    "else:\n",
    "    venues_raw = fetch_venues_osm(PLACE_NAMES, VENUE_TAGS)\n",
    "    save_geojson(venues_raw, VENUES_RAW_PATH)\n",
    "\n",
    "    venues = clean_venues_osm(venues_raw)\n",
    "    save_geojson(venues, VENUES_CLEAN_PATH)\n",
    "\n",
    "venues.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ed03f07",
   "metadata": {},
   "source": [
    "## A2: DEFRA strategic road noise via OGC API Features\n",
    "\n",
    "We retrieve DEFRA road noise polygons using the official OGC API Features service.\n",
    "\n",
    "Why this method is defensible:\n",
    "* It uses an **official API endpoint** rather than time limited file links.\n",
    "* The query is bounded to the study area using a venue based Area of Interest.\n",
    "* The output is stored as a GeoPackage for stable downstream processing.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "15a9b312",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad5a005c7b8a4ce8a132a105b3a46cff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "DEFRA pages: 0page [00:00, ?page/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEFRA AOI noise saved\n",
      "Total features: 53137\n",
      "Output file: /home/sai/test/notebook_test/data/raw/defra_noise/defra_road_noise_lden_round3_aoi.gpkg\n",
      "\n",
      "Manual verification instructions\n",
      "Source page: https://environment.data.gov.uk/dataset/fd1c6327-ad77-42ae-a761-7c6a0866523d\n",
      "Dataset: Road_Noise_Lden_England_Round_3\n",
      "Look for a GeoPackage download option if you want to cross check the API output\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PosixPath('/home/sai/test/notebook_test/data/raw/defra_noise/defra_road_noise_lden_round3_aoi.gpkg')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datetime import datetime, timezone\n",
    "\n",
    "# Paths\n",
    "DEFRA_DIR = RAW_DIR / \"defra_noise\"\n",
    "DEFRA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "DEFRA_OUT_GPKG = DEFRA_DIR / \"defra_road_noise_lden_round3_aoi.gpkg\"\n",
    "DEFRA_LAYER = \"defra_road_noise_lden_round3_aoi\"\n",
    "DEFRA_PROVENANCE = DEFRA_DIR / \"provenance_defra.json\"\n",
    "\n",
    "# AOI buffer expands beyond venues to include local context\n",
    "AOI_BUFFER_M = 2500\n",
    "\n",
    "# Set True if you want to force a fresh pull from the API\n",
    "REFRESH_DEFRA = False\n",
    "\n",
    "# DEFRA dataset identifiers\n",
    "DATASET_ID = \"fd1c6327-ad77-42ae-a761-7c6a0866523d\"\n",
    "COLLECTION_ID = \"Road_Noise_Lden_England_Round_3\"\n",
    "\n",
    "OGC_BASE = f\"https://environment.data.gov.uk/geoservices/datasets/{DATASET_ID}/ogc/features/v1\"\n",
    "ITEMS_URL = f\"{OGC_BASE}/collections/{COLLECTION_ID}/items\"\n",
    "SOURCE_PAGE_URL = f\"https://environment.data.gov.uk/dataset/{DATASET_ID}\"\n",
    "\n",
    "def now_utc_iso():\n",
    "    return datetime.now(timezone.utc).isoformat()\n",
    "\n",
    "def load_json_safe(path: Path):\n",
    "    try:\n",
    "        if path.exists():\n",
    "            return json.loads(path.read_text(encoding=\"utf-8\"))\n",
    "    except Exception:\n",
    "        pass\n",
    "    return []\n",
    "\n",
    "def save_json(path: Path, obj):\n",
    "    path.write_text(json.dumps(obj, indent=2), encoding=\"utf-8\")\n",
    "\n",
    "def valid_gpkg(path: Path):\n",
    "    return path.exists() and path.stat().st_size > 0\n",
    "\n",
    "def print_manual_defra_instructions():\n",
    "    print(\"\")\n",
    "    print(\"Manual verification instructions\")\n",
    "    print(\"Source page:\", SOURCE_PAGE_URL)\n",
    "    print(\"Dataset:\", COLLECTION_ID)\n",
    "    print(\"Look for a GeoPackage download option if you want to cross check the API output\")\n",
    "\n",
    "def build_aoi_from_venues(venues_path: Path, buffer_m: float):\n",
    "    v = gpd.read_file(venues_path)\n",
    "    if v.crs is None:\n",
    "        raise ValueError(\"venues CRS missing\")\n",
    "    v = v.to_crs(\"EPSG:27700\")\n",
    "    try:\n",
    "        geom = v.geometry.union_all()\n",
    "    except Exception:\n",
    "        geom = v.unary_union\n",
    "    aoi = geom.convex_hull.buffer(float(buffer_m))\n",
    "    return aoi, aoi.bounds\n",
    "\n",
    "def fetch_all_features_bbox(bbox_27700, limit=10000, timeout_s=90):\n",
    "    # Bounding box query in EPSG:27700\n",
    "    params = {\n",
    "        \"bbox\": \",\".join([str(x) for x in bbox_27700]),\n",
    "        \"bbox-crs\": \"http://www.opengis.net/def/crs/EPSG/0/27700\",\n",
    "        \"crs\": \"http://www.opengis.net/def/crs/EPSG/0/27700\",\n",
    "        \"limit\": int(limit),\n",
    "        \"f\": \"application/geo+json\",\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        from tqdm.auto import tqdm\n",
    "        use_tqdm = True\n",
    "    except Exception:\n",
    "        use_tqdm = False\n",
    "\n",
    "    features = []\n",
    "    url = ITEMS_URL\n",
    "    page = 0\n",
    "\n",
    "    pbar = None\n",
    "    if use_tqdm:\n",
    "        pbar = tqdm(desc=\"DEFRA pages\", unit=\"page\")\n",
    "\n",
    "    while True:\n",
    "        page += 1\n",
    "        r = requests.get(url, params=params, timeout=timeout_s)\n",
    "        r.raise_for_status()\n",
    "        js = r.json()\n",
    "\n",
    "        chunk = js.get(\"features\", [])\n",
    "        features.extend(chunk)\n",
    "\n",
    "        next_url = None\n",
    "        for link in js.get(\"links\", []):\n",
    "            if link.get(\"rel\") == \"next\":\n",
    "                next_url = link.get(\"href\")\n",
    "                break\n",
    "\n",
    "        if use_tqdm:\n",
    "            pbar.update(1)\n",
    "            pbar.set_postfix({\n",
    "                \"page\": page,\n",
    "                \"page_features\": len(chunk),\n",
    "                \"total_features\": len(features),\n",
    "            })\n",
    "        else:\n",
    "            print(\"DEFRA page\", page, \"features\", len(chunk), \"total\", len(features))\n",
    "\n",
    "        if not next_url:\n",
    "            break\n",
    "\n",
    "        # After the first page the next URL usually includes params\n",
    "        url = next_url\n",
    "        params = None\n",
    "\n",
    "        # Safety stop to avoid infinite loops\n",
    "        if page > 500:\n",
    "            raise RuntimeError(\"Too many pages returned\")\n",
    "\n",
    "    if use_tqdm:\n",
    "        pbar.close()\n",
    "\n",
    "    return features\n",
    "\n",
    "def ensure_defra_noise_aoi(venues_path: Path):\n",
    "    provenance = load_json_safe(DEFRA_PROVENANCE)\n",
    "\n",
    "    # Cache hit\n",
    "    if valid_gpkg(DEFRA_OUT_GPKG) and not REFRESH_DEFRA:\n",
    "        print(\"DEFRA noise already present, skipping download\")\n",
    "        print(\"Output file:\", DEFRA_OUT_GPKG)\n",
    "        print_manual_defra_instructions()\n",
    "\n",
    "        provenance.append({\n",
    "            \"dataset\": \"DEFRA Road Noise Lden Round 3\",\n",
    "            \"method\": \"ogc_api_features_bbox\",\n",
    "            \"status\": \"already_present\",\n",
    "            \"checked_at_utc\": now_utc_iso(),\n",
    "            \"output\": str(DEFRA_OUT_GPKG)\n",
    "        })\n",
    "        save_json(DEFRA_PROVENANCE, provenance)\n",
    "        return DEFRA_OUT_GPKG\n",
    "\n",
    "    # Build AOI from venues and query using its bounding box\n",
    "    _aoi_geom, bbox = build_aoi_from_venues(venues_path, AOI_BUFFER_M)\n",
    "\n",
    "    t0 = time.time()\n",
    "    features = fetch_all_features_bbox(bbox)\n",
    "    if not features:\n",
    "        raise RuntimeError(\"No DEFRA features returned\")\n",
    "\n",
    "    gdf = gpd.GeoDataFrame.from_features(features, crs=\"EPSG:27700\")\n",
    "    gdf = gdf[gdf.geometry.notna() & ~gdf.geometry.is_empty].copy()\n",
    "\n",
    "    # Overwrite if present\n",
    "    if DEFRA_OUT_GPKG.exists():\n",
    "        DEFRA_OUT_GPKG.unlink()\n",
    "\n",
    "    gdf.to_file(DEFRA_OUT_GPKG, layer=DEFRA_LAYER, driver=\"GPKG\")\n",
    "\n",
    "    provenance.append({\n",
    "        \"dataset\": \"DEFRA Road Noise Lden Round 3\",\n",
    "        \"method\": \"ogc_api_features_bbox\",\n",
    "        \"status\": \"downloaded\",\n",
    "        \"downloaded_at_utc\": now_utc_iso(),\n",
    "        \"source_items_url\": ITEMS_URL,\n",
    "        \"source_page\": SOURCE_PAGE_URL,\n",
    "        \"aoi_buffer_m\": AOI_BUFFER_M,\n",
    "        \"bbox_27700\": [float(x) for x in bbox],\n",
    "        \"n_features\": int(len(gdf)),\n",
    "        \"runtime_s\": float(time.time() - t0),\n",
    "        \"output\": str(DEFRA_OUT_GPKG)\n",
    "    })\n",
    "    save_json(DEFRA_PROVENANCE, provenance)\n",
    "\n",
    "    print(\"DEFRA AOI noise saved\")\n",
    "    print(\"Total features:\", len(gdf))\n",
    "    print(\"Output file:\", DEFRA_OUT_GPKG)\n",
    "    print_manual_defra_instructions()\n",
    "    return DEFRA_OUT_GPKG\n",
    "\n",
    "# Run acquisition\n",
    "defra_noise_gpkg = ensure_defra_noise_aoi(VENUES_CLEAN_PATH)\n",
    "defra_noise_gpkg\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10cf4e1d",
   "metadata": {},
   "source": [
    "## A3: Sentinel 2 NDVI from CEDA and summer median composite\n",
    "\n",
    "We build a robust summer NDVI composite to represent vegetation.\n",
    "\n",
    "Approach:\n",
    "* Infer the Sentinel tile code from the venue centroid\n",
    "* For each target month, select one valid tile image\n",
    "* Run integrity checks to avoid partial or corrupted TIFF downloads\n",
    "* Clip each NDVI image to the venue based Area of Interest\n",
    "* Compute a **median** composite across selected summer images\n",
    "\n",
    "Note: \n",
    "NDVI and strategic road noise layers represent slowly varying structural environmental conditions.\n",
    "Venue locations are used as spatial anchors to sample these conditions rather than as time specific observations.\n",
    "The framework therefore estimates structural comfort potential rather than historical realised comfort at a specific year.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ac516ed7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Manual verification instructions\n",
      "Source directory: https://data.ceda.ac.uk/neodc/sentinel_ard/products/indices/sentinel_2/ndvi/v2//2017/\n",
      "Tile: T30UXC (Note:Inner London, including Camden and Islington, falls inside the Sentinel 2 tile T30UXC.)\n",
      "Target months: [6, 7, 8]\n",
      "If you need manual download, place NDVI GeoTIFFs into:\n",
      "  /home/sai/test/notebook_test/data/raw/ndvi/files\n",
      "\n",
      "Scanning 2017 06\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94c749113c1347c3894e7a576ac84653",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Scan 2017-06:   0%|          | 0/18 [00:00<?, ?day/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1784a0fd0c75449ea8c876020bd45a83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "S2A_20170601_lat52lon075_T30UXC_ORB137_utm30n_osgb_NDVI.tif:   0%|          | 0.00/505M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Candidate: 2017-06-01 S2A_20170601_lat52lon075_T30UXC_ORB137_utm30n_osgb_NDVI.tif downloaded\n",
      "Selected for composite: S2A_20170601_lat52lon075_T30UXC_ORB137_utm30n_osgb_NDVI.tif\n",
      "\n",
      "Scanning 2017 07\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7b5d18fef2548909cfb1fd7da60514e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Scan 2017-07:   0%|          | 0/25 [00:00<?, ?day/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33a3246790d7441f81e6ddf505e462bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "S2A_20170731_lat52lon075_T30UXC_ORB137_utm30n_osgb_NDVI.tif:   0%|          | 0.00/516M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Candidate: 2017-07-31 S2A_20170731_lat52lon075_T30UXC_ORB137_utm30n_osgb_NDVI.tif downloaded\n",
      "Selected for composite: S2A_20170731_lat52lon075_T30UXC_ORB137_utm30n_osgb_NDVI.tif\n",
      "\n",
      "Scanning 2017 08\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "522d90abfe6e4030a29ed691be07a504",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Scan 2017-08:   0%|          | 0/27 [00:00<?, ?day/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88c76a0fd6ea43ffadee624e2e30fb3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "S2B_20170805_lat52lon075_T30UXC_ORB137_utm30n_osgb_NDVI.tif:   0%|          | 0.00/490M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Candidate: 2017-08-05 S2B_20170805_lat52lon075_T30UXC_ORB137_utm30n_osgb_NDVI.tif downloaded\n",
      "Selected for composite: S2B_20170805_lat52lon075_T30UXC_ORB137_utm30n_osgb_NDVI.tif\n",
      "\n",
      "Building median composite from 3 images\n",
      "Saved NDVI composite: /home/sai/test/notebook_test/data/raw/ndvi/ndvi_median_summer_2017_T30UXC_aoi.tif\n",
      "Note: NDVI is extracted for 2017 to maintain temporal consistency with the DEFRA noise dataset.\n"
     ]
    }
   ],
   "source": [
    "# Settings\n",
    "YEAR = 2017\n",
    "MONTHS_TARGET = [6, 7, 8]\n",
    "WORKERS = 6\n",
    "\n",
    "AOI_BUFFER_M = 2500\n",
    "CEDA_BASE = \"https://dap.ceda.ac.uk/neodc/sentinel_ard/products/indices/sentinel_2/ndvi/v2\"\n",
    "SOURCE_URL = \"https://data.ceda.ac.uk/neodc/sentinel_ard/products/indices/sentinel_2/ndvi/v2/\"\n",
    "\n",
    "NDVI_DIR = RAW_DIR / \"ndvi\"\n",
    "NDVI_FILES_DIR = NDVI_DIR / \"files\"\n",
    "NDVI_DIR.mkdir(parents=True, exist_ok=True)\n",
    "NDVI_FILES_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "NDVI_PROVENANCE = NDVI_DIR / \"provenance_ndvi.json\"\n",
    "\n",
    "# Set True if you want to rebuild the composite\n",
    "REFRESH_NDVI = False\n",
    "\n",
    "LIST_TIMEOUT = (10, 25)\n",
    "DL_TIMEOUT = (20, 900)\n",
    "CHUNK = 1024 * 1024\n",
    "\n",
    "def now_utc_iso():\n",
    "    return datetime.now(timezone.utc).isoformat()\n",
    "\n",
    "def sha256_file(path: Path):\n",
    "    h = hashlib.sha256()\n",
    "    with Path(path).open(\"rb\") as f:\n",
    "        for chunk in iter(lambda: f.read(1024 * 1024), b\"\"):\n",
    "            h.update(chunk)\n",
    "    return h.hexdigest()\n",
    "\n",
    "def load_json_safely(path: Path):\n",
    "    try:\n",
    "        if path.exists():\n",
    "            return json.loads(path.read_text(encoding=\"utf-8\"))\n",
    "    except Exception:\n",
    "        return []\n",
    "    return []\n",
    "\n",
    "def save_json(path: Path, obj):\n",
    "    path.write_text(json.dumps(obj, indent=2), encoding=\"utf-8\")\n",
    "\n",
    "def valid_file(path: Path):\n",
    "    return path.exists() and path.stat().st_size > 0\n",
    "\n",
    "def print_manual_ndvi_instructions(tile_code: str):\n",
    "    print(\"\")\n",
    "    print(\"Manual verification instructions\")\n",
    "    print(\"Source directory:\", f\"{SOURCE_URL}/{YEAR:04d}/\")\n",
    "    print(\"Tile:\", f\"T{tile_code}\" \" \" \"(Note:Inner London, including Camden and Islington, falls inside the Sentinel 2 tile T30UXC.)\" )\n",
    "    print(\"Target months:\", MONTHS_TARGET)\n",
    "    print(\"If you need manual download, place NDVI GeoTIFFs into:\")\n",
    "    print(\" \", str(NDVI_FILES_DIR))\n",
    "\n",
    "def make_session():\n",
    "    s = requests.Session()\n",
    "    retry = Retry(total=5, backoff_factor=0.5, status_forcelist=[429, 500, 502, 503, 504])\n",
    "    adapter = HTTPAdapter(max_retries=retry)\n",
    "    s.mount(\"https://\", adapter)\n",
    "    s.mount(\"http://\", adapter)\n",
    "    return s\n",
    "\n",
    "def get_aoi_geometry_from_venues(venues_path: Path, buffer_m: float):\n",
    "    v = gpd.read_file(venues_path)\n",
    "    if v.crs is None:\n",
    "        raise ValueError(\"venues CRS missing\")\n",
    "    v = v.to_crs(\"EPSG:27700\")\n",
    "    try:\n",
    "        geom = v.geometry.union_all()\n",
    "    except Exception:\n",
    "        geom = v.unary_union\n",
    "    return geom.convex_hull.buffer(float(buffer_m))\n",
    "\n",
    "def infer_mgrs_tile_from_venues(venues_path: Path):\n",
    "    v = gpd.read_file(venues_path)\n",
    "    if v.crs is None:\n",
    "        raise ValueError(\"venues CRS missing\")\n",
    "    v = v.to_crs(\"EPSG:4326\")\n",
    "    try:\n",
    "        geom = v.geometry.union_all()\n",
    "    except Exception:\n",
    "        geom = v.unary_union\n",
    "    c = geom.centroid\n",
    "    m = mgrs.MGRS()\n",
    "    code = m.toMGRS(c.y, c.x, MGRSPrecision=0)\n",
    "    if isinstance(code, bytes):\n",
    "        code = code.decode(\"utf-8\")\n",
    "    return str(code)\n",
    "\n",
    "def robust_get_text(session: requests.Session, url: str, timeout=LIST_TIMEOUT):\n",
    "    r = session.get(url, timeout=timeout)\n",
    "    r.raise_for_status()\n",
    "    return r.text\n",
    "\n",
    "def list_tif_links(html: str, base_url: str):\n",
    "    links = re.findall(r'href=\"([^\"]+\\.tif)\"', html, flags=re.IGNORECASE)\n",
    "    out = []\n",
    "    for lnk in links:\n",
    "        if lnk.startswith(\"http\"):\n",
    "            out.append(lnk)\n",
    "        else:\n",
    "            out.append(base_url.rstrip(\"/\") + \"/\" + lnk.lstrip(\"/\"))\n",
    "    return out\n",
    "\n",
    "def list_days_in_month(session: requests.Session, year: int, month: int):\n",
    "    month_url = f\"{CEDA_BASE}/{year:04d}/{month:02d}/\"\n",
    "    html = robust_get_text(session, month_url)\n",
    "    return sorted({int(d) for d in re.findall(r'href=\"(\\d{2})\\/\"', html)})\n",
    "\n",
    "def find_all_tile_files_for_day(session: requests.Session, tile: str, year: int, month: int, day: int):\n",
    "    day_url = f\"{CEDA_BASE}/{year:04d}/{month:02d}/{day:02d}/\"\n",
    "    try:\n",
    "        html = robust_get_text(session, day_url)\n",
    "        tifs = list_tif_links(html, day_url)\n",
    "        matches = [u for u in tifs if f\"_T{tile}_\" in u and \"NDVI\" in u.upper()]\n",
    "        if matches:\n",
    "            return date(year, month, day), matches\n",
    "    except Exception:\n",
    "        return None\n",
    "    return None\n",
    "\n",
    "def scan_month_for_tile_candidates(session: requests.Session, tile: str, year: int, month: int, workers: int = 6):\n",
    "    days = list_days_in_month(session, year, month)\n",
    "\n",
    "    try:\n",
    "        from tqdm.auto import tqdm\n",
    "        use_tqdm = True\n",
    "    except Exception:\n",
    "        use_tqdm = False\n",
    "\n",
    "    found = []\n",
    "    with ThreadPoolExecutor(max_workers=int(workers)) as ex:\n",
    "        futs = [ex.submit(find_all_tile_files_for_day, session, tile, year, month, d) for d in days]\n",
    "        it = as_completed(futs)\n",
    "        if use_tqdm:\n",
    "            it = tqdm(it, total=len(futs), desc=f\"Scan {year}-{month:02d}\", unit=\"day\")\n",
    "        for f in it:\n",
    "            out = f.result()\n",
    "            if out:\n",
    "                found.append(out)\n",
    "\n",
    "    if not found:\n",
    "        return []\n",
    "\n",
    "    found.sort(key=lambda x: x[0])\n",
    "    candidates = []\n",
    "    for d, urls in found:\n",
    "        for u in urls:\n",
    "            candidates.append((d, u))\n",
    "    return candidates\n",
    "\n",
    "def raster_read_test(tif_path: Path):\n",
    "    \"\"\"Quick integrity test to catch partial or corrupted TIFFs.\"\"\"\n",
    "    if not valid_file(tif_path):\n",
    "        return False\n",
    "    try:\n",
    "        with rasterio.open(tif_path) as src:\n",
    "            h = int(src.height)\n",
    "            w = int(src.width)\n",
    "            if h <= 0 or w <= 0:\n",
    "                return False\n",
    "            row0 = max(0, (h // 2) - 8)\n",
    "            col0 = max(0, (w // 2) - 8)\n",
    "            window = rasterio.windows.Window(col0, row0, 16, 16)\n",
    "            _ = src.read(1, window=window, boundless=True)\n",
    "        return True\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "def download_file_with_progress(session: requests.Session, url: str, out_path: Path):\n",
    "    \"\"\"Download a single file with retry and a post download integrity check.\"\"\"\n",
    "    out_path = Path(out_path)\n",
    "\n",
    "    # Cache hit\n",
    "    if valid_file(out_path) and raster_read_test(out_path):\n",
    "        return \"already_present\", sha256_file(out_path)\n",
    "\n",
    "    # Remove broken file before retry\n",
    "    if out_path.exists():\n",
    "        try:\n",
    "            out_path.unlink()\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    try:\n",
    "        from tqdm.auto import tqdm\n",
    "        use_tqdm = True\n",
    "    except Exception:\n",
    "        use_tqdm = False\n",
    "\n",
    "    with session.get(url, stream=True, timeout=DL_TIMEOUT) as r:\n",
    "        r.raise_for_status()\n",
    "        total = r.headers.get(\"Content-Length\")\n",
    "        total = int(total) if total and total.isdigit() else None\n",
    "\n",
    "        pbar = None\n",
    "        if use_tqdm:\n",
    "            pbar = tqdm(total=total, unit=\"B\", unit_scale=True, desc=out_path.name, leave=False)\n",
    "\n",
    "        with out_path.open(\"wb\") as f:\n",
    "            for chunk in r.iter_content(chunk_size=CHUNK):\n",
    "                if not chunk:\n",
    "                    continue\n",
    "                f.write(chunk)\n",
    "                if pbar is not None:\n",
    "                    pbar.update(len(chunk))\n",
    "\n",
    "        if pbar is not None:\n",
    "            pbar.close()\n",
    "\n",
    "    # Validate\n",
    "    if not raster_read_test(out_path):\n",
    "        try:\n",
    "            out_path.unlink()\n",
    "        except Exception:\n",
    "            pass\n",
    "        return \"download_failed_or_corrupt\", None\n",
    "\n",
    "    return \"downloaded\", sha256_file(out_path)\n",
    "\n",
    "def write_median_composite(tif_paths, aoi_geom, out_tif: Path):\n",
    "    \"\"\"Clip inputs to AOI and write a median NDVI composite.\"\"\"\n",
    "    arrays = []\n",
    "    meta = None\n",
    "    bad = []\n",
    "\n",
    "    for p in tif_paths:\n",
    "        p = Path(p)\n",
    "        if not raster_read_test(p):\n",
    "            bad.append(p.name)\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            with rasterio.open(p) as src:\n",
    "                clipped, transform = mask(src, [aoi_geom], crop=True)\n",
    "                arr = clipped[0].astype(np.float32)\n",
    "\n",
    "                nodata = src.nodata\n",
    "                if nodata is not None:\n",
    "                    arr = np.where(arr == nodata, np.nan, arr)\n",
    "\n",
    "                arrays.append(arr)\n",
    "\n",
    "                if meta is None:\n",
    "                    meta = src.meta.copy()\n",
    "                    meta.update(\n",
    "                        height=clipped.shape[1],\n",
    "                        width=clipped.shape[2],\n",
    "                        transform=transform,\n",
    "                        nodata=-9999,\n",
    "                        dtype=\"float32\",\n",
    "                        count=1,\n",
    "                    )\n",
    "        except (RasterioIOError, Exception):\n",
    "            bad.append(p.name)\n",
    "            continue\n",
    "\n",
    "    if len(arrays) == 0:\n",
    "        raise RuntimeError(\"All NDVI inputs failed integrity checks, cannot build composite\")\n",
    "\n",
    "    stack = np.stack(arrays)\n",
    "    med = np.nanmedian(stack, axis=0)\n",
    "    med = np.where(np.isfinite(med), med, -9999).astype(np.float32)\n",
    "\n",
    "    out_tif = Path(out_tif)\n",
    "    with rasterio.open(out_tif, \"w\", **meta) as dst:\n",
    "        dst.write(med, 1)\n",
    "\n",
    "    return bad\n",
    "\n",
    "def ensure_ndvi_composite():\n",
    "    tile = infer_mgrs_tile_from_venues(VENUES_CLEAN_PATH)\n",
    "    aoi = get_aoi_geometry_from_venues(VENUES_CLEAN_PATH, AOI_BUFFER_M)\n",
    "    out_tif = NDVI_DIR / f\"ndvi_median_summer_{YEAR}_T{tile}_aoi.tif\"\n",
    "\n",
    "    prov = load_json_safely(NDVI_PROVENANCE)\n",
    "\n",
    "    # Always print manual instructions for auditability\n",
    "    print_manual_ndvi_instructions(tile)\n",
    "\n",
    "    # Cache hit\n",
    "    if valid_file(out_tif) and not REFRESH_NDVI:\n",
    "        print(\"\")\n",
    "        print(\"NDVI composite already present, skipping build\")\n",
    "        print(\"Output file:\", out_tif)\n",
    "        prov.append({\n",
    "            \"created_at_utc\": now_utc_iso(),\n",
    "            \"status\": \"already_present\",\n",
    "            \"year\": YEAR,\n",
    "            \"months\": MONTHS_TARGET,\n",
    "            \"tile\": tile,\n",
    "            \"aoi_buffer_m\": AOI_BUFFER_M,\n",
    "            \"output_composite\": str(out_tif)\n",
    "        })\n",
    "        save_json(NDVI_PROVENANCE, prov)\n",
    "        return out_tif\n",
    "\n",
    "    session = make_session()\n",
    "    downloads = []\n",
    "    used = []\n",
    "\n",
    "    for mth in MONTHS_TARGET:\n",
    "        print(\"\")\n",
    "        print(\"Scanning\", YEAR, f\"{mth:02d}\")\n",
    "        candidates = scan_month_for_tile_candidates(session, tile, YEAR, mth, workers=WORKERS)\n",
    "\n",
    "        if not candidates:\n",
    "            print(\"No tile match found in this month\")\n",
    "            continue\n",
    "\n",
    "        chosen = None\n",
    "        for d, url in candidates:\n",
    "            fname = url.split(\"/\")[-1].split(\"?\")[0]\n",
    "            out_path = NDVI_FILES_DIR / fname\n",
    "\n",
    "            status, fhash = download_file_with_progress(session, url, out_path)\n",
    "            used.append({\n",
    "                \"month\": int(mth),\n",
    "                \"selected_date\": str(d),\n",
    "                \"url\": url,\n",
    "                \"file\": fname,\n",
    "                \"status\": status,\n",
    "                \"sha256\": fhash\n",
    "            })\n",
    "\n",
    "            print(\"Candidate:\", d, fname, status)\n",
    "\n",
    "            if status in [\"already_present\", \"downloaded\"]:\n",
    "                chosen = out_path\n",
    "                break\n",
    "\n",
    "        if chosen is None:\n",
    "            print(\"All candidates failed for this month\")\n",
    "            continue\n",
    "\n",
    "        downloads.append(chosen)\n",
    "        print(\"Selected for composite:\", chosen.name)\n",
    "\n",
    "    if len(downloads) == 0:\n",
    "        raise RuntimeError(\"No valid NDVI files available, cannot create composite\")\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"Building median composite from\", len(downloads), \"images\")\n",
    "    t0 = time.time()\n",
    "    bad_inputs = write_median_composite(downloads, aoi, out_tif)\n",
    "    runtime_s = float(time.time() - t0)\n",
    "\n",
    "    prov.append({\n",
    "        \"created_at_utc\": now_utc_iso(),\n",
    "        \"status\": \"built\",\n",
    "        \"year\": YEAR,\n",
    "        \"months\": MONTHS_TARGET,\n",
    "        \"tile\": tile,\n",
    "        \"aoi_buffer_m\": AOI_BUFFER_M,\n",
    "        \"downloaded_files\": used,\n",
    "        \"output_composite\": str(out_tif),\n",
    "        \"output_sha256\": sha256_file(out_tif),\n",
    "        \"build_runtime_s\": runtime_s,\n",
    "        \"bad_inputs_skipped\": bad_inputs\n",
    "    })\n",
    "    save_json(NDVI_PROVENANCE, prov)\n",
    "\n",
    "    print(\"Saved NDVI composite:\", out_tif)\n",
    "    if bad_inputs:\n",
    "        print(\"Skipped corrupted inputs:\", bad_inputs)\n",
    "\n",
    "    return out_tif\n",
    "\n",
    "ndvi_composite_path = ensure_ndvi_composite()\n",
    "ndvi_composite_path\n",
    "\n",
    "\n",
    "print(\"Note: NDVI is extracted for 2017 to maintain temporal consistency with the DEFRA noise dataset.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3e5b534",
   "metadata": {},
   "source": [
    "## A4 Verification summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b856fd58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking outputs\n",
      "Venues clean exists: True /home/sai/test/notebook_test/data/processed/venues_clean.geojson\n",
      "DEFRA gpkg count: 1 /home/sai/test/notebook_test/data/raw/defra_noise\n",
      "NDVI composite exists: True /home/sai/test/notebook_test/data/raw/ndvi/ndvi_median_summer_2017_T30UXC_aoi.tif\n",
      "\n",
      "NDVI composites in folder:\n",
      " - ndvi_median_summer_2017_T30UXC_aoi.tif\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 4 Verification summary\n",
    "# ============================================================\n",
    "\n",
    "def list_gpkgs(folder: Path):\n",
    "    folder = Path(folder)\n",
    "    return sorted([p for p in folder.glob(\"*.gpkg\") if p.exists() and p.stat().st_size > 0])\n",
    "\n",
    "print(\"Checking outputs\")\n",
    "print(\"Venues clean exists:\", VENUES_CLEAN_PATH.exists(), str(VENUES_CLEAN_PATH))\n",
    "print(\"DEFRA gpkg count:\", len(list_gpkgs(DEFRA_DIR)), str(DEFRA_DIR))\n",
    "print(\"NDVI composite exists:\", Path(ndvi_composite_path).exists(), str(ndvi_composite_path))\n",
    "\n",
    "print(\"\")\n",
    "print(\"NDVI composites in folder:\")\n",
    "for p in sorted((RAW_DIR / \"ndvi\").glob(\"ndvi_median_summer_*.tif\")):\n",
    "    print(\" -\", p.name)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dft",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
